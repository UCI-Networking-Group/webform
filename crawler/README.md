## Step 2: Web Form Crawler

In Section 3.1 (Web Form Crawler) of our paper, we describe the specialized crawler we developed to discover web forms on websites. The code for this web form crawler is provided in this folder.

### Step 2.1: Running the Crawler

To discover and download web forms from a website, execute `node build/crawler.js` with the output path and starting URL as arguments. For example:

```console
$ cd crawler
$ node build/crawler.js ~/webform-data/uci.edu/ http://uci.edu/
```

By default, the crawler will stop after completing 100 tasks. You can modify this setting using the `--maxJobCount N` argument, where `N` is the desired number of tasks.

Upon completion, the output folder (`~/webform-data/uci.edu/` in this example) will contain subfolders with long hexadecimal names, as shown below:

```text
webform-data/uci.edu/
├── ......
├── 9c76eced8de70e1376355a554f37755cfc0e5b7259bdf70cf3cba31b66f0a79f
│   ├── form-0.json
│   ├── form-0.png
│   ├── form-1.json
│   ├── form-1.png
│   ├── job.json
│   ├── next-steps.json
│   ├── page.html
│   └── page.png
└── ......
```

Each subfolder corresponds to a *crawl task*, as explained in the paper. The files within each subfolder include:

- `job-*.json`: Metadata of the crawl task, including page title, navigation history, etc.
- `form-*.json`: Information about the web forms discovered on the webpage, including HTML code for the forms and their fields.
- `page.html`: A copy of the entire webpage's HTML code.
- `page.png`, `form-*.png`: Screenshots of the webpage and the forms, primarily for debugging purposes (screenshots may be missing if the form was not visible in the viewport).
- `next-steps.json`: Information about new crawl tasks identified on the current page, primarily for debugging purposes.

### Step 2.2: Creating the Web Form Dataset

In [Step 1.4](../website-list/README.md), you obtained a list of domains (`<DOMAIN>`) and homepage URLs (`<HOMEPAGE_URL>`) to crawl. We use the crawler to collect the raw web form dataset by running the crawler command for each domain and URL:

```console
$ node build/crawler.js ~/webform-data/<DOMAIN_1> <HOMEPAGE_URL_1>
$ node build/crawler.js ~/webform-data/<DOMAIN_2> <HOMEPAGE_URL_2>
$ node build/crawler.js ~/webform-data/<DOMAIN_3> <HOMEPAGE_URL_3>
......
```

### Artifacts

The raw web form dataset can be found in our released artifacts:

- `crawl-merged-core.tar.zst` -- Essential files for the subsequent data processing steps.
- `extra/crawl-merged-extra-*.tar.xz` -- Additional files (`*.png`, `next-steps.json`) generated by the crawler. These files are large and unlikely to be useful. We keep them for archival purposes.

To restore the dataset:

```console
$ mkdir ~/webform-data
$ tar xf crawl-merged-core.tar.zst -C ~/webform-data
```

Due to the large number of files in the dataset, the extraction process can take a considerable amount of time, potentially tens of minutes, depending on your hardware.
